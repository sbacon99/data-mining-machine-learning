{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pca_lab.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNFBY2wej3nNZ6EsHuJqbOF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CL6sYzbDXtxN"},"source":["# Lab 12: PCA\n","## Sam Bacon - March 29, 2021\n","### Using Principal Component Analysis for Dimensionality Reduction"]},{"cell_type":"code","metadata":{"id":"v5V4oS_jYV1p"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"oFDXNfqfXnwN","executionInfo":{"status":"ok","timestamp":1617045266318,"user_tz":240,"elapsed":1803,"user":{"displayName":"Sam Bacon","photoUrl":"","userId":"00763127992609288573"}},"outputId":"8f58c2cb-e2ed-42ff-b9e7-869a93c67f95"},"source":["# read in data\n","data_csv = 'https://drive.google.com/uc?export=download&id=1vqMBid4r0C8apwYcpBQfVQ_TwEUNWY8r'\n","data = pd.read_csv(data_csv)\n","data.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.536741</td>\n","      <td>-3.301172</td>\n","      <td>-0.401756</td>\n","      <td>-1.202885</td>\n","      <td>1.634385</td>\n","      <td>0.328210</td>\n","      <td>0.509842</td>\n","      <td>0.307943</td>\n","      <td>-1.330579</td>\n","      <td>0.690989</td>\n","      <td>2.056854</td>\n","      <td>4.245124</td>\n","      <td>-1.122276</td>\n","      <td>-1.286053</td>\n","      <td>0.570713</td>\n","      <td>0.332914</td>\n","      <td>0.065116</td>\n","      <td>1.965521</td>\n","      <td>-2.677922</td>\n","      <td>0.145463</td>\n","      <td>-0.765567</td>\n","      <td>-0.108297</td>\n","      <td>1.063411</td>\n","      <td>-0.438486</td>\n","      <td>-1.151013</td>\n","      <td>1.475069</td>\n","      <td>0.997267</td>\n","      <td>2.782064</td>\n","      <td>1.896763</td>\n","      <td>-0.039366</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.700367</td>\n","      <td>-2.672759</td>\n","      <td>-0.683936</td>\n","      <td>-2.410570</td>\n","      <td>1.046541</td>\n","      <td>0.066310</td>\n","      <td>-0.602418</td>\n","      <td>1.015624</td>\n","      <td>-2.061458</td>\n","      <td>0.174439</td>\n","      <td>1.189218</td>\n","      <td>3.816560</td>\n","      <td>-0.600531</td>\n","      <td>1.093787</td>\n","      <td>0.323073</td>\n","      <td>0.176187</td>\n","      <td>1.493695</td>\n","      <td>2.650732</td>\n","      <td>-3.260990</td>\n","      <td>0.149962</td>\n","      <td>-0.141244</td>\n","      <td>0.271675</td>\n","      <td>2.083393</td>\n","      <td>0.119871</td>\n","      <td>0.704316</td>\n","      <td>1.143842</td>\n","      <td>-1.723951</td>\n","      <td>2.419307</td>\n","      <td>1.033950</td>\n","      <td>-0.033414</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.020962</td>\n","      <td>-0.851870</td>\n","      <td>1.270827</td>\n","      <td>-0.921015</td>\n","      <td>-1.205010</td>\n","      <td>-0.552378</td>\n","      <td>0.009268</td>\n","      <td>0.409418</td>\n","      <td>0.627190</td>\n","      <td>-0.683375</td>\n","      <td>0.076625</td>\n","      <td>-1.163256</td>\n","      <td>0.459435</td>\n","      <td>-0.642715</td>\n","      <td>0.376824</td>\n","      <td>0.466007</td>\n","      <td>-0.018943</td>\n","      <td>-0.668694</td>\n","      <td>-0.867758</td>\n","      <td>-1.384927</td>\n","      <td>0.455188</td>\n","      <td>-2.445057</td>\n","      <td>-0.628649</td>\n","      <td>-0.592058</td>\n","      <td>0.149045</td>\n","      <td>1.808181</td>\n","      <td>0.636952</td>\n","      <td>1.196119</td>\n","      <td>1.751840</td>\n","      <td>1.279966</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.450912</td>\n","      <td>-1.156441</td>\n","      <td>-2.247361</td>\n","      <td>1.744520</td>\n","      <td>-1.183791</td>\n","      <td>-1.837285</td>\n","      <td>3.915360</td>\n","      <td>0.129572</td>\n","      <td>1.739377</td>\n","      <td>1.295061</td>\n","      <td>5.047583</td>\n","      <td>2.291222</td>\n","      <td>3.047817</td>\n","      <td>2.329900</td>\n","      <td>-1.681013</td>\n","      <td>2.591229</td>\n","      <td>-0.429771</td>\n","      <td>0.237015</td>\n","      <td>0.372583</td>\n","      <td>-0.389226</td>\n","      <td>0.087047</td>\n","      <td>-0.059553</td>\n","      <td>-2.438575</td>\n","      <td>-1.691106</td>\n","      <td>-0.258106</td>\n","      <td>-0.306145</td>\n","      <td>-0.884099</td>\n","      <td>1.796661</td>\n","      <td>0.033870</td>\n","      <td>0.763104</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.239896</td>\n","      <td>2.216549</td>\n","      <td>-2.511707</td>\n","      <td>0.581304</td>\n","      <td>1.978495</td>\n","      <td>0.272168</td>\n","      <td>-1.235817</td>\n","      <td>-0.958662</td>\n","      <td>-2.358019</td>\n","      <td>1.147567</td>\n","      <td>-1.565297</td>\n","      <td>1.967257</td>\n","      <td>-0.618975</td>\n","      <td>0.037661</td>\n","      <td>-1.619541</td>\n","      <td>-1.517656</td>\n","      <td>-0.840908</td>\n","      <td>1.651650</td>\n","      <td>1.042925</td>\n","      <td>2.301618</td>\n","      <td>0.242111</td>\n","      <td>-0.232808</td>\n","      <td>2.306771</td>\n","      <td>2.040415</td>\n","      <td>-0.339121</td>\n","      <td>-0.049949</td>\n","      <td>0.015628</td>\n","      <td>-2.624473</td>\n","      <td>-4.741839</td>\n","      <td>-0.050563</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          0         1         2         3  ...        27        28        29   30\n","0  2.536741 -3.301172 -0.401756 -1.202885  ...  2.782064  1.896763 -0.039366  0.0\n","1  2.700367 -2.672759 -0.683936 -2.410570  ...  2.419307  1.033950 -0.033414  0.0\n","2 -1.020962 -0.851870  1.270827 -0.921015  ...  1.196119  1.751840  1.279966  1.0\n","3 -1.450912 -1.156441 -2.247361  1.744520  ...  1.796661  0.033870  0.763104  0.0\n","4  2.239896  2.216549 -2.511707  0.581304  ... -2.624473 -4.741839 -0.050563  0.0\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"dQP5BCUTYh8_"},"source":["# split data\n","X = data.drop(data.columns[30], axis = 1)\n","y = data.iloc[:,30]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KukJLTvbZh8"},"source":["# train/test data\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.5, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4q1ZkpTDb1Ar","executionInfo":{"status":"ok","timestamp":1617045823012,"user_tz":240,"elapsed":715,"user":{"displayName":"Sam Bacon","photoUrl":"","userId":"00763127992609288573"}},"outputId":"494e9c41-682d-4790-f281-313e4c6f620c"},"source":["# define PCA\n","pca = PCA(n_components=2)\n","pca.fit(Xtrain)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n","    svd_solver='auto', tol=0.0, whiten=False)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xuA3yWk9cRsf","executionInfo":{"status":"ok","timestamp":1617046641908,"user_tz":240,"elapsed":494,"user":{"displayName":"Sam Bacon","photoUrl":"","userId":"00763127992609288573"}},"outputId":"498eea80-a302-4bca-8c75-048c26604db0"},"source":["# transform Xtrain\n","Xtrain2 = pca.transform(Xtrain)\n","print(\"shape:\", Xtrain2.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["shape: (500, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GVXP1JCocueb","executionInfo":{"status":"ok","timestamp":1617046026152,"user_tz":240,"elapsed":763,"user":{"displayName":"Sam Bacon","photoUrl":"","userId":"00763127992609288573"}},"outputId":"a4074173-630b-4755-dd0a-cbebe37458cd"},"source":["# explained variance ration\n","pca.explained_variance_ratio_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.35609967, 0.19444318])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA3WTJlPdDRz","executionInfo":{"status":"ok","timestamp":1617046436496,"user_tz":240,"elapsed":513,"user":{"displayName":"Sam Bacon","photoUrl":"","userId":"00763127992609288573"}},"outputId":"bf6f87d9-a1da-4c1c-b708-28e3b183ef21"},"source":["# KNN model on Xtrain\n","model = KNeighborsClassifier(n_neighbors=1)\n","model.fit(Xtrain, ytrain)\n","ypred = model.predict(Xtest)\n","\n","print(classification_report(ytest, ypred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Asg3BPkdVt4","executionInfo":{"status":"ok","timestamp":1617046450336,"user_tz":240,"elapsed":887,"user":{"displayName":"Sam Bacon","photoUrl":"","userId":"00763127992609288573"}},"outputId":"f9d2f47a-73fc-4640-abaf-6d2bf4e8923a"},"source":["# KNN model on Xtrain2\n","model_t = KNeighborsClassifier(n_neighbors=1)\n","model_t.fit(Xtrain2, ytrain)\n","\n","Xtest2 = pca.transform(Xtest)\n","ypred_t = model_t.predict(Xtest2)\n","\n","print(classification_report(ytest, ypred_t))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.59      0.58      0.59       250\n","         1.0       0.59      0.60      0.59       250\n","\n","    accuracy                           0.59       500\n","   macro avg       0.59      0.59      0.59       500\n","weighted avg       0.59      0.59      0.59       500\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fd_n2S6qfoFB"},"source":["The KNN model had an accuracy of 0.94 when predicting Xtest using Xtrain, but it only had an accuracy of 0.59 when attempting to predict Xtest2 using Xtrain2. This decrease in accuracy is not necessarily surprising, but it is a clear indication that the dimensionality reduction has removed attributes from the training data that are useful for predicting ytest. It would probably make sense to increase the number of components to increase accuracy. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzaJAzDogdys","executionInfo":{"status":"ok","timestamp":1617047800443,"user_tz":240,"elapsed":2010,"user":{"displayName":"Sam Bacon","photoUrl":"","userId":"00763127992609288573"}},"outputId":"2649ef10-e3b4-4ffa-9d76-daa47c0bd093"},"source":["# experimenting with different number of components\n","\n","# components = 3\n","for i in np.arange(2,31):\n","  pca = PCA(n_components=i)\n","  pca.fit(Xtrain)\n","  Xtrain2 = pca.transform(Xtrain)\n","  model_t = KNeighborsClassifier(n_neighbors=1)\n","  model_t.fit(Xtrain2, ytrain)\n","  Xtest2 = pca.transform(Xtest)\n","  ypred_t = model_t.predict(Xtest2)\n","  print(\"n_components =\", i)\n","  print(classification_report(ytest, ypred_t))\n","  print(\"Cumulative variance explained by model:\")\n","  print(np.cumsum(pca.explained_variance_ratio_))\n","  print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["n_components = 2\n","              precision    recall  f1-score   support\n","\n","         0.0       0.59      0.58      0.59       250\n","         1.0       0.59      0.60      0.59       250\n","\n","    accuracy                           0.59       500\n","   macro avg       0.59      0.59      0.59       500\n","weighted avg       0.59      0.59      0.59       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285]\n","\n","n_components = 3\n","              precision    recall  f1-score   support\n","\n","         0.0       0.90      0.88      0.89       250\n","         1.0       0.88      0.91      0.89       250\n","\n","    accuracy                           0.89       500\n","   macro avg       0.89      0.89      0.89       500\n","weighted avg       0.89      0.89      0.89       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931 ]\n","\n","n_components = 4\n","              precision    recall  f1-score   support\n","\n","         0.0       0.91      0.88      0.89       250\n","         1.0       0.89      0.91      0.90       250\n","\n","    accuracy                           0.90       500\n","   macro avg       0.90      0.90      0.90       500\n","weighted avg       0.90      0.90      0.90       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526 ]\n","\n","n_components = 5\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.96      0.96       250\n","         1.0       0.96      0.95      0.96       250\n","\n","    accuracy                           0.96       500\n","   macro avg       0.96      0.96      0.96       500\n","weighted avg       0.96      0.96      0.96       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231]\n","\n","n_components = 6\n","              precision    recall  f1-score   support\n","\n","         0.0       0.96      0.96      0.96       250\n","         1.0       0.96      0.96      0.96       250\n","\n","    accuracy                           0.96       500\n","   macro avg       0.96      0.96      0.96       500\n","weighted avg       0.96      0.96      0.96       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349]\n","\n","n_components = 7\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.97      0.96       250\n","         1.0       0.97      0.95      0.96       250\n","\n","    accuracy                           0.96       500\n","   macro avg       0.96      0.96      0.96       500\n","weighted avg       0.96      0.96      0.96       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921]\n","\n","n_components = 8\n","              precision    recall  f1-score   support\n","\n","         0.0       0.96      0.96      0.96       250\n","         1.0       0.96      0.96      0.96       250\n","\n","    accuracy                           0.96       500\n","   macro avg       0.96      0.96      0.96       500\n","weighted avg       0.96      0.96      0.96       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616]\n","\n","n_components = 9\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.96      0.96       250\n","         1.0       0.96      0.95      0.96       250\n","\n","    accuracy                           0.96       500\n","   macro avg       0.96      0.96      0.96       500\n","weighted avg       0.96      0.96      0.96       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183]\n","\n","n_components = 10\n","              precision    recall  f1-score   support\n","\n","         0.0       0.96      0.94      0.95       250\n","         1.0       0.94      0.96      0.95       250\n","\n","    accuracy                           0.95       500\n","   macro avg       0.95      0.95      0.95       500\n","weighted avg       0.95      0.95      0.95       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151]\n","\n","n_components = 11\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.95      0.95       250\n","         1.0       0.95      0.95      0.95       250\n","\n","    accuracy                           0.95       500\n","   macro avg       0.95      0.95      0.95       500\n","weighted avg       0.95      0.95      0.95       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104]\n","\n","n_components = 12\n","              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.96      0.95       250\n","         1.0       0.96      0.95      0.95       250\n","\n","    accuracy                           0.95       500\n","   macro avg       0.95      0.95      0.95       500\n","weighted avg       0.95      0.95      0.95       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925]\n","\n","n_components = 13\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.96      0.95       250\n","         1.0       0.96      0.94      0.95       250\n","\n","    accuracy                           0.95       500\n","   macro avg       0.95      0.95      0.95       500\n","weighted avg       0.95      0.95      0.95       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244]\n","\n","n_components = 14\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.96      0.95       250\n","         1.0       0.96      0.93      0.95       250\n","\n","    accuracy                           0.95       500\n","   macro avg       0.95      0.95      0.95       500\n","weighted avg       0.95      0.95      0.95       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539]\n","\n","n_components = 15\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.        ]\n","\n","n_components = 16\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.        ]\n","\n","n_components = 17\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.        ]\n","\n","n_components = 18\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.        ]\n","\n","n_components = 19\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.        ]\n","\n","n_components = 20\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.        ]\n","\n","n_components = 21\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.        ]\n","\n","n_components = 22\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.        ]\n","\n","n_components = 23\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.        ]\n","\n","n_components = 24\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.        ]\n","\n","n_components = 25\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.\n"," 1.        ]\n","\n","n_components = 26\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.\n"," 1.         1.        ]\n","\n","n_components = 27\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.\n"," 1.         1.         1.        ]\n","\n","n_components = 28\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.\n"," 1.         1.         1.         1.        ]\n","\n","n_components = 29\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.        ]\n","\n","n_components = 30\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94       250\n","         1.0       0.95      0.93      0.94       250\n","\n","    accuracy                           0.94       500\n","   macro avg       0.94      0.94      0.94       500\n","weighted avg       0.94      0.94      0.94       500\n","\n","Cumulative variance explained by model:\n","[0.35609967 0.55054285 0.7000931  0.8047526  0.86787231 0.88424349\n"," 0.89954921 0.91361616 0.92730183 0.94067151 0.95356104 0.96586925\n"," 0.97788244 0.98921539 1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.\n"," 1.         1.         1.         1.         1.         1.        ]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T3J10C6sil3a"},"source":["The highest accuracy obtained was 0.96, which occured when n_components = 5-9. For these models, all of the variance in the original data set is explained by the components (see cumulative explained variances above). Therefore, we clearly do not need all 30 attributes to accurately predict ytest."]},{"cell_type":"code","metadata":{"id":"gHTGIn25j6uz"},"source":[""],"execution_count":null,"outputs":[]}]}